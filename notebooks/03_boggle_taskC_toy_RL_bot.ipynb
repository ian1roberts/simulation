{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task C — Toy Reinforcement Learning: A Boggle Bot with Shaped Rewards\n",
    "\n",
    "**Goal:** Train a small policy with **REINFORCE** to explore a 3×3 board and stop when it has spelt a valid 3–5 letter word. Rewards are shaped via the dictionary **prefix trie** to mitigate sparsity.\n",
    "\n",
    "This is a pedagogical toy loop, not a performance benchmark. It illustrates:\n",
    "- State encoding,\n",
    "- Masked action spaces,\n",
    "- Reward shaping,\n",
    "- Learning curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Dictionary, trie, and boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse loader\n",
    "DICT_CANDIDATES = [\n",
    "    \"/usr/share/dict/british-english\",\n",
    "    \"/usr/share/dict/words\",\n",
    "    \"/usr/share/dict/american-english\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_words(paths=DICT_CANDIDATES, min_len=3, max_len=5):\n",
    "    words = set()\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for w in f:\n",
    "                    w = w.strip().lower()\n",
    "                    if w.isalpha() and min_len <= len(w) <= max_len:\n",
    "                        words.add(w)\n",
    "    if not words:\n",
    "        words = {\n",
    "            \"cat\", \"cats\", \"cater\", \"art\", \"arts\", \"rat\", \"rate\", \"rates\",\n",
    "            \"tar\", \"tars\", \"tire\", \"tired\", \"ride\", \"rides\", \"ear\", \"ears\",\n",
    "            \"earl\", \"ale\", \"ales\", \"tea\", \"teas\", \"eat\", \"eats\", \"seat\",\n",
    "            \"sear\", \"scar\", \"care\", \"cared\"\n",
    "        }\n",
    "    return sorted(words)\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    __slots__ = (\"children\", \"is_word\")\n",
    "\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, words):\n",
    "        self.root = TrieNode()\n",
    "        for w in words:\n",
    "            self.add(w)\n",
    "\n",
    "    def add(self, w):\n",
    "        n = self.root\n",
    "        for ch in w:\n",
    "            n = n.children.setdefault(ch, TrieNode())\n",
    "        n.is_word = True\n",
    "\n",
    "    def has_prefix(self, prefix):\n",
    "        n = self.root\n",
    "        for ch in prefix:\n",
    "            n = n.children.get(ch)\n",
    "            if n is None:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def is_word(self, w):\n",
    "        n = self.root\n",
    "        for ch in w:\n",
    "            n = n.children.get(ch)\n",
    "            if n is None:\n",
    "                return False\n",
    "        return n.is_word\n",
    "\n",
    "\n",
    "WORDS = load_words()\n",
    "TRIE = Trie(WORDS)\n",
    "\n",
    "# Simple board sampler (frequency-based)\n",
    "LETTER_FREQ = {\n",
    "    \"e\": 0.127, \"t\": 0.091, \"a\": 0.082, \"o\": 0.075, \"i\": 0.070, \"n\": 0.067,\n",
    "    \"s\": 0.063, \"h\": 0.061, \"r\": 0.060, \"d\": 0.043, \"l\": 0.040, \"c\": 0.028,\n",
    "    \"u\": 0.028, \"m\": 0.024, \"w\": 0.024, \"f\": 0.022, \"g\": 0.020, \"y\": 0.020,\n",
    "    \"p\": 0.019, \"b\": 0.015, \"v\": 0.010, \"k\": 0.008, \"j\": 0.002, \"x\": 0.002,\n",
    "    \"q\": 0.001, \"z\": 0.001,\n",
    "}\n",
    "LETTERS = list(LETTER_FREQ.keys())\n",
    "PROBS = np.array([LETTER_FREQ[c] for c in LETTERS])\n",
    "PROBS = PROBS / PROBS.sum()\n",
    "\n",
    "\n",
    "def sample_board():\n",
    "    return \"\".join(np.random.choice(LETTERS, size=9, p=PROBS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Environment (masked actions, shaped rewards)\n",
    "- Actions: 0..8 select cell, 9 = STOP.\n",
    "- State: board letters, used mask, last position, current prefix.\n",
    "- Masks: cannot revisit cells; must be adjacent except on the first move.\n",
    "- Rewards:\n",
    "  - +1.0 if STOP with a valid word (length 3–5),\n",
    "  - +0.1 for staying within a valid prefix,\n",
    "  - −0.1 when prefix falls out of the trie,\n",
    "  - small −0.01 step cost to encourage shorter words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX2RC = [(r, c) for r in range(3) for c in range(3)]\n",
    "RC2IDX = {rc: i for i, rc in enumerate(IDX2RC)}\n",
    "\n",
    "\n",
    "def neighbours(idx):\n",
    "    r, c = IDX2RC[idx]\n",
    "    nbrs = []\n",
    "    for dr in (-1, 0, 1):\n",
    "        for dc in (-1, 0, 1):\n",
    "            if dr == 0 and dc == 0:\n",
    "                continue\n",
    "            rr, cc = r + dr, c + dc\n",
    "            if 0 <= rr < 3 and 0 <= cc < 3:\n",
    "                nbrs.append(RC2IDX[(rr, cc)])\n",
    "    return nbrs\n",
    "\n",
    "\n",
    "class BoggleEnv:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = sample_board()\n",
    "        self.used = np.zeros(9, dtype=np.float32)\n",
    "        self.pos = -1\n",
    "        self.prefix = \"\"\n",
    "        self.t = 0\n",
    "        return self._obs()\n",
    "\n",
    "    def _mask(self):\n",
    "        legal = np.ones(10, dtype=np.float32)  # 9 cells + STOP\n",
    "        for i in range(9):\n",
    "            if self.used[i] == 1.0:\n",
    "                legal[i] = 0.0\n",
    "        if self.t > 0:\n",
    "            adj = np.zeros(9, dtype=np.float32)\n",
    "            for j in neighbours(self.pos):\n",
    "                adj[j] = 1.0\n",
    "            for i in range(9):\n",
    "                legal[i] *= adj[i]\n",
    "        if self.t == 0:\n",
    "            legal[9] = 0.0\n",
    "        return legal\n",
    "\n",
    "    def _obs(self):\n",
    "        def onehot(c):\n",
    "            v = np.zeros(26, dtype=np.float32)\n",
    "            v[ord(c) - 97] = 1.0\n",
    "            return v\n",
    "\n",
    "        board_vec = np.concatenate([onehot(c) for c in self.board], axis=0)\n",
    "        used = self.used.astype(np.float32)\n",
    "        pos = np.zeros(10, dtype=np.float32)\n",
    "        pos[self.pos + 1] = 1.0  # -1 -> index 0\n",
    "        pref = np.zeros(26, dtype=np.float32)\n",
    "        for ch in self.prefix:\n",
    "            pref[ord(ch) - 97] += 1.0\n",
    "        pref = pref / (np.sum(pref) + 1e-6)\n",
    "        t_vec = np.zeros(6, dtype=np.float32)\n",
    "        t_vec[self.t] = 1.0\n",
    "        return np.concatenate([board_vec, used, pos, pref, t_vec], axis=0), self._mask()\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -0.01  # step cost\n",
    "        done = False\n",
    "        info = {}\n",
    "        if action == 9:  # STOP\n",
    "            if 3 <= len(self.prefix) <= 5 and TRIE.is_word(self.prefix):\n",
    "                reward += 1.0\n",
    "            done = True\n",
    "            return self._obs(), reward, done, info\n",
    "        mask = self._mask()\n",
    "        if mask[action] == 0.0:\n",
    "            return self._obs(), -0.1, True, {\"illegal\": True}\n",
    "        self.used[action] = 1.0\n",
    "        self.pos = action\n",
    "        self.prefix += self.board[action]\n",
    "        self.t += 1\n",
    "        if not TRIE.has_prefix(self.prefix):\n",
    "            reward -= 0.1\n",
    "        else:\n",
    "            reward += 0.1\n",
    "        if self.t >= 5:\n",
    "            done = True\n",
    "            if TRIE.is_word(self.prefix):\n",
    "                reward += 1.0\n",
    "        return self._obs(), reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Policy network and REINFORCE\n",
    "We use a small MLP that outputs 10 logits (9 cells + STOP). We apply the mask by subtracting a large constant from illegal logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = len(BoggleEnv()._obs()[0])\n",
    "ACT_DIM = 10\n",
    "\n",
    "\n",
    "def make_policy(hidden=256):\n",
    "    inp = keras.Input(shape=(OBS_DIM,), dtype=\"float32\")\n",
    "    x = layers.Dense(hidden, activation=\"relu\")(inp)\n",
    "    x = layers.Dense(hidden // 2, activation=\"relu\")(x)\n",
    "    logits = layers.Dense(ACT_DIM)(x)\n",
    "    return keras.Model(inp, logits)\n",
    "\n",
    "\n",
    "policy = make_policy()\n",
    "optimizer = keras.optimizers.Adam(1e-3)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def reinforce_update(obs_batch, act_batch, ret_batch, mask_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = policy(obs_batch, training=True)\n",
    "        masked_logits = logits + (1.0 - mask_batch) * (-1e9)\n",
    "        logp = tf.nn.log_softmax(masked_logits)\n",
    "        act_onehot = tf.one_hot(act_batch, depth=ACT_DIM)\n",
    "        logp_act = tf.reduce_sum(act_onehot * logp, axis=1)\n",
    "        loss = -tf.reduce_mean(logp_act * ret_batch)\n",
    "    grads = tape.gradient(loss, policy.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, policy.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Training loop & learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, gamma=0.99):\n",
    "    obs_list = []\n",
    "    act_list = []\n",
    "    rew_list = []\n",
    "    mask_list = []\n",
    "    obs, mask = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        logits = policy(obs[None, :], training=False).numpy()[0]\n",
    "        masked_logits = logits + (1.0 - mask) * (-1e9)\n",
    "        probs = tf.nn.softmax(masked_logits).numpy()\n",
    "        a = int(np.random.choice(np.arange(ACT_DIM), p=probs))\n",
    "        obs_list.append(obs)\n",
    "        act_list.append(a)\n",
    "        mask_list.append(mask)\n",
    "        (obs, mask), r, done, info = env.step(a)\n",
    "        rew_list.append(r)\n",
    "    G = 0.0\n",
    "    rets = []\n",
    "    for r in reversed(rew_list):\n",
    "        G = r + gamma * G\n",
    "        rets.append(G)\n",
    "    rets = list(reversed(rets))\n",
    "    return (\n",
    "        np.array(obs_list),\n",
    "        np.array(act_list),\n",
    "        np.array(rets, dtype=np.float32),\n",
    "        np.array(mask_list),\n",
    "    )\n",
    "\n",
    "\n",
    "env = BoggleEnv()\n",
    "EPISODES = 400\n",
    "mean_returns = []\n",
    "success_rates = []\n",
    "\n",
    "for ep in range(1, EPISODES + 1):\n",
    "    obs, act, ret, mask = run_episode(env)\n",
    "    # simple baseline: normalise returns\n",
    "    ret = (ret - ret.mean()) / (ret.std() + 1e-6)\n",
    "    loss = reinforce_update(\n",
    "        tf.constant(obs, dtype=tf.float32),\n",
    "        tf.constant(act, dtype=tf.int32),\n",
    "        tf.constant(ret, dtype=tf.float32),\n",
    "        tf.constant(mask, dtype=tf.float32),\n",
    "    )\n",
    "    mean_returns.append(float(ret.mean()))\n",
    "    # success if final reward included +1 (word found); approximate check via last raw return > 0.9\n",
    "    success = 1.0 if ret[-1] > 0.9 else 0.0\n",
    "    success_rates.append(success)\n",
    "    if ep % 50 == 0:\n",
    "        print(\n",
    "            f\"Episode {ep}  loss={float(loss):.4f}  mean_norm_return={mean_returns[-1]:.3f}  success≈{success}\"\n",
    "        )\n",
    "\n",
    "# Plot learning signals\n",
    "plt.figure()\n",
    "plt.plot(np.convolve(success_rates, np.ones(20) / 20, mode=\"valid\"))\n",
    "plt.title(\"Smoothed success indicator (~word found)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Success (rolling mean)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- The policy improves faster if you **initialise** it from the supervised next-move model (Task B) — a classic *behaviour cloning → RL fine-tune* pipeline.\n",
    "- Try alternative shaping, different step costs, or curriculum boards to see how exploration dynamics change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
