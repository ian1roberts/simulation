{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task A — Oracle, Dataset, and Baseline Classifier (Keras/TensorFlow)\n",
    "\n",
    "**Goal:** Build a classical solver (trie + DFS) for a 3×3 board, generate a labelled dataset of *(board, word, path)* examples, and train a simple **baseline classifier** that predicts whether a *path exists* for a given (board, word) pair.\n",
    "\n",
    "This notebook is for Python-savvy learners who are new to deep learning. We emphasise:\n",
    "- Exact search as a ground truth **oracle**.\n",
    "- Clean data generation that **avoids leakage**.\n",
    "- A simple Keras baseline that we will **beat** in later notebooks with path-decoding models and RL.\n",
    "\n",
    "> Adjacency: we default to **8-neighbour** (like Boggle), but you can switch to 4-neighbour via a flag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical & plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Dictionary loading (Ubuntu `british-english`)\n",
    "We try typical system paths and fall back to a tiny demo list if none are found.\n",
    "\n",
    "- Filters: lower-case alphabetic words of length 3–5.\n",
    "- Builds a **prefix trie** for fast prefix checks (used by the oracle and later by RL for reward shaping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_CANDIDATES = [\n",
    "    \"/usr/share/dict/british-english\",\n",
    "    \"/usr/share/dict/words\",\n",
    "    \"/usr/share/dict/american-english\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_words(paths=DICT_CANDIDATES, min_len=3, max_len=5):\n",
    "    words = set()\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for w in f:\n",
    "                    w = w.strip().lower()\n",
    "                    if w.isalpha() and min_len <= len(w) <= max_len:\n",
    "                        words.add(w)\n",
    "    if not words:\n",
    "        # Fallback mini list to keep the notebook runnable anywhere.\n",
    "        words = {\n",
    "            \"cat\",\n",
    "            \"cats\",\n",
    "            \"cater\",\n",
    "            \"art\",\n",
    "            \"arts\",\n",
    "            \"rat\",\n",
    "            \"rate\",\n",
    "            \"rates\",\n",
    "            \"tar\",\n",
    "            \"tars\",\n",
    "            \"tire\",\n",
    "            \"tired\",\n",
    "            \"ride\",\n",
    "            \"rides\",\n",
    "            \"ear\",\n",
    "            \"ears\",\n",
    "            \"earl\",\n",
    "            \"ale\",\n",
    "            \"ales\",\n",
    "            \"tea\",\n",
    "            \"teas\",\n",
    "            \"eat\",\n",
    "            \"eats\",\n",
    "            \"seat\",\n",
    "            \"sear\",\n",
    "            \"scar\",\n",
    "            \"care\",\n",
    "            \"cared\",\n",
    "        }\n",
    "    return sorted(words)\n",
    "\n",
    "\n",
    "WORDS = load_words()\n",
    "print(f\"Loaded {len(WORDS)} words (3–5 letters). Sample: {WORDS[:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple Trie for prefix and word checks\n",
    "class TrieNode:\n",
    "    __slots__ = (\"children\", \"is_word\")\n",
    "\n",
    "    def __init__(self):\n",
    "        self.children = dict()\n",
    "        self.is_word = False\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, words):\n",
    "        self.root = TrieNode()\n",
    "        for w in words:\n",
    "            self.add(w)\n",
    "\n",
    "    def add(self, word):\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            node = node.children.setdefault(ch, TrieNode())\n",
    "        node.is_word = True\n",
    "\n",
    "    def has_prefix(self, prefix):\n",
    "        node = self.root\n",
    "        for ch in prefix:\n",
    "            node = node.children.get(ch)\n",
    "            if node is None:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def is_word(self, word):\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            node = node.children.get(ch)\n",
    "            if node is None:\n",
    "                return False\n",
    "        return node.is_word\n",
    "\n",
    "\n",
    "TRIE = Trie(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Board representation and oracle DFS\n",
    "- Boards are 3×3 arrays of letters: length‑9 strings or lists are fine.\n",
    "- Choose **8-neighbour** (default) or **4-neighbour** adjacency.\n",
    "- Oracle returns **(word, path)** pairs where *path* is a list of cell indices [0..8].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Board utilities\n",
    "IDX2RC = [(r, c) for r in range(3) for c in range(3)]\n",
    "RC2IDX = {rc: i for i, rc in enumerate(IDX2RC)}\n",
    "\n",
    "\n",
    "def neighbours(idx, use_diagonals=True):\n",
    "    r, c = IDX2RC[idx]\n",
    "    nbrs = []\n",
    "    for dr in (-1, 0, 1):\n",
    "        for dc in (-1, 0, 1):\n",
    "            if dr == 0 and dc == 0:\n",
    "                continue\n",
    "            if not use_diagonals and abs(dr) + abs(dc) != 1:\n",
    "                continue\n",
    "            rr, cc = r + dr, c + dc\n",
    "            if 0 <= rr < 3 and 0 <= cc < 3:\n",
    "                nbrs.append(RC2IDX[(rr, cc)])\n",
    "    return nbrs\n",
    "\n",
    "\n",
    "def draw_board(board):\n",
    "    # board: iterable of 9 characters\n",
    "    grid = np.array(board).reshape(3, 3)\n",
    "    print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "\n",
    "\n",
    "def plot_board(board, path=None, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.set_xlim(-0.5, 2.5)\n",
    "    ax.set_ylim(-0.5, 2.5)\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_yticks(range(3))\n",
    "    ax.grid(True)\n",
    "    for idx, ch in enumerate(board):\n",
    "        r, c = IDX2RC[idx]\n",
    "        ax.text(c, 2 - r, ch, ha=\"center\", va=\"center\", fontsize=18)\n",
    "    if path:\n",
    "        xs = [IDX2RC[i][1] for i in path]\n",
    "        ys = [2 - r for r in [IDX2RC[i][0] for i in path]]\n",
    "        ax.plot(xs, ys, marker=\"o\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect(\"equal\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_words_on_board(board, use_diagonals=True, min_len=3, max_len=5):\n",
    "    board = list(board)\n",
    "    results = []  # (word, path)\n",
    "    used = [False] * 9\n",
    "\n",
    "    def dfs(idx, prefix, path):\n",
    "        ch = board[idx]\n",
    "        new_prefix = prefix + ch\n",
    "        if not TRIE.has_prefix(new_prefix):\n",
    "            return\n",
    "        used[idx] = True\n",
    "        path.append(idx)\n",
    "        if TRIE.is_word(new_prefix) and (min_len <= len(new_prefix) <= max_len):\n",
    "            results.append((\"\".join(new_prefix), path.copy()))\n",
    "        for nb in neighbours(idx, use_diagonals=use_diagonals):\n",
    "            if not used[nb]:\n",
    "                dfs(nb, new_prefix, path)\n",
    "        path.pop()\n",
    "        used[idx] = False\n",
    "\n",
    "    for start in range(9):\n",
    "        dfs(start, \"\", [])\n",
    "    # De-duplicate identical words with different paths by keeping all paths (useful for training)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "board_demo = list(\"ratecides\")  # r a t / e c i / d e s\n",
    "print(\"Demo board:\")\n",
    "draw_board(board_demo)\n",
    "res = oracle_words_on_board(board_demo, use_diagonals=True)\n",
    "print(f\"Oracle found {len(res)} (word, path) pairs; first few:\", res[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Board sampling\n",
    "We generate random boards. For realism, you may bias letters by English frequency. Here we support both modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Frequency from a rough approximation (can be refined); sums to 1.0\n",
    "LETTER_FREQ = {\n",
    "    \"e\": 0.127,\n",
    "    \"t\": 0.091,\n",
    "    \"a\": 0.082,\n",
    "    \"o\": 0.075,\n",
    "    \"i\": 0.070,\n",
    "    \"n\": 0.067,\n",
    "    \"s\": 0.063,\n",
    "    \"h\": 0.061,\n",
    "    \"r\": 0.060,\n",
    "    \"d\": 0.043,\n",
    "    \"l\": 0.040,\n",
    "    \"c\": 0.028,\n",
    "    \"u\": 0.028,\n",
    "    \"m\": 0.024,\n",
    "    \"w\": 0.024,\n",
    "    \"f\": 0.022,\n",
    "    \"g\": 0.020,\n",
    "    \"y\": 0.020,\n",
    "    \"p\": 0.019,\n",
    "    \"b\": 0.015,\n",
    "    \"v\": 0.010,\n",
    "    \"k\": 0.008,\n",
    "    \"j\": 0.002,\n",
    "    \"x\": 0.002,\n",
    "    \"q\": 0.001,\n",
    "    \"z\": 0.001,\n",
    "}\n",
    "LETTERS = sorted(LETTER_FREQ.keys())\n",
    "PROBS = np.array([LETTER_FREQ[c] for c in LETTERS], dtype=float)\n",
    "PROBS = PROBS / PROBS.sum()\n",
    "\n",
    "\n",
    "def sample_board(n=1, mode=\"frequency\"):\n",
    "    boards = []\n",
    "    for _ in range(n):\n",
    "        if mode == \"uniform\":\n",
    "            b = np.random.choice(list(\"abcdefghijklmnopqrstuvwxyz\"), size=(9,), replace=True)\n",
    "        else:\n",
    "            b = np.random.choice(LETTERS, size=(9,), p=PROBS, replace=True)\n",
    "        boards.append(list(b))\n",
    "    return boards\n",
    "\n",
    "\n",
    "print(\"Sampled boards (first 2):\")\n",
    "for b in sample_board(2):\n",
    "    draw_board(b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 4. Dataset construction\n",
    "We produce a dataset of **(board, word, path)** and also create **negative pairs** (board, word with no valid path).\n",
    "\n",
    "- Train/Val/Test split at the **board level** to prevent leakage.\n",
    "- Save to `data/boggle_3x3.npz` for reuse in later notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    num_boards=250, mode=\"frequency\", use_diagonals=True, min_len=3, max_len=5, neg_ratio=1.0\n",
    "):\n",
    "    boards = sample_board(num_boards, mode=mode)\n",
    "    positive = []  # (board_str, word, path)\n",
    "    neg_pairs = set()\n",
    "    for b in boards:\n",
    "        pairs = oracle_words_on_board(\n",
    "            b, use_diagonals=use_diagonals, min_len=min_len, max_len=max_len\n",
    "        )\n",
    "        for w, p in pairs:\n",
    "            positive.append((\"\".join(b), w, p))\n",
    "        # Generate negatives by sampling random words of 3–5 letters with no path in this board\n",
    "        if neg_ratio > 0:\n",
    "            need = int(len(pairs) * neg_ratio) + 3  # ensure some negatives even if pairs is small\n",
    "            tries = 0\n",
    "            while need > 0 and tries < 5000:\n",
    "                tries += 1\n",
    "                w = random.choice(WORDS)\n",
    "                # Quick check: if oracle didn't list it, it's negative\n",
    "                if not any(w == wp for wp, _ in pairs):\n",
    "                    neg_pairs.add((\"\".join(b), w))\n",
    "                    need -= 1\n",
    "    random.shuffle(positive)\n",
    "    neg_list = list(neg_pairs)\n",
    "    random.shuffle(neg_list)\n",
    "    return boards, positive, neg_list\n",
    "\n",
    "\n",
    "boards, positive, negatives = build_dataset(num_boards=300)\n",
    "print(\n",
    "    f\"Dataset sizes -> boards:{len(boards)}, positives:{len(positive)}, negatives:{len(negatives)}\"\n",
    ")\n",
    "print(\"Example positive:\", positive[:2])\n",
    "print(\"Example negative:\", negatives[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by boards to avoid leakage: 70/15/15\n",
    "def split_by_boards(boards, positives, negatives, train=0.7, val=0.15, test=0.15):\n",
    "    board_strs = [\"\".join(b) for b in boards]\n",
    "    random.shuffle(board_strs)\n",
    "    n = len(board_strs)\n",
    "    n_tr = int(n * train)\n",
    "    n_va = int(n * val)\n",
    "    tr_boards = set(board_strs[:n_tr])\n",
    "    va_boards = set(board_strs[n_tr : n_tr + n_va])\n",
    "    te_boards = set(board_strs[n_tr + n_va :])\n",
    "\n",
    "    def filt_pos(pos_list, Bset):\n",
    "        return [x for x in pos_list if x[0] in Bset]\n",
    "\n",
    "    def filt_neg(neg_list, Bset):\n",
    "        return [x for x in neg_list if x[0] in Bset]\n",
    "\n",
    "    ds = {\n",
    "        \"train_pos\": filt_pos(positives, tr_boards),\n",
    "        \"val_pos\": filt_pos(positives, va_boards),\n",
    "        \"test_pos\": filt_pos(positives, te_boards),\n",
    "        \"train_neg\": filt_neg(negatives, tr_boards),\n",
    "        \"val_neg\": filt_neg(negatives, va_boards),\n",
    "        \"test_neg\": filt_neg(negatives, te_boards),\n",
    "    }\n",
    "    for k, v in ds.items():\n",
    "        print(k, len(v))\n",
    "    return ds\n",
    "\n",
    "\n",
    "ds = split_by_boards(boards, positive, negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Vectorisation helpers\n",
    "We encode letters as integers. Words are padded to length **5**. Boards are length‑9 sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ALPH = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "C2I = {c: i + 1 for i, c in enumerate(ALPH)}  # 0 is PAD\n",
    "I2C = {i: c for c, i in C2I.items()}\n",
    "PAD = 0\n",
    "MAX_WORD_LEN = 5\n",
    "BOARD_LEN = 9\n",
    "\n",
    "\n",
    "def encode_word(w):\n",
    "    arr = [C2I.get(ch, PAD) for ch in w.lower()]\n",
    "    if len(arr) < MAX_WORD_LEN:\n",
    "        arr += [PAD] * (MAX_WORD_LEN - len(arr))\n",
    "    return np.array(arr, dtype=np.int32)\n",
    "\n",
    "\n",
    "def encode_board(bstr):\n",
    "    arr = [C2I.get(ch, PAD) for ch in bstr.lower()]\n",
    "    return np.array(arr, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 6. Baseline classifier (does a path exist?)\n",
    "\n",
    "**Important:** This baseline *does not* produce a path; it only predicts if the word is findable on the board. It can “cheat” by ignoring geometry (e.g., treating the board as a bag of letters). In **Task B**, we’ll train a pointer-style model that **outputs the path** step-by-step with masking and teacher forcing.\n",
    "\n",
    "Architecture:\n",
    "- Inputs: encoded board (9 tokens) and word (padded to 5).\n",
    "- Embeddings → small Transformer encoder blocks (or BiLSTMs).\n",
    "- Concatenate pooled features → MLP → sigmoid (exist / not-exist).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_baseline_model(vocab_size=len(C2I) + 1, d_model=64):\n",
    "    board_in = keras.Input(shape=(BOARD_LEN,), dtype=\"int32\", name=\"board\")\n",
    "    word_in = keras.Input(shape=(MAX_WORD_LEN,), dtype=\"int32\", name=\"word\")\n",
    "\n",
    "    emb = layers.Embedding(input_dim=vocab_size, output_dim=d_model, mask_zero=True)\n",
    "    b = emb(board_in)  # (None, 9, d)\n",
    "    w = emb(word_in)  # (None, 5, d)\n",
    "\n",
    "    # Simple encoders (choose LSTM for clarity)\n",
    "    b_enc = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(b)\n",
    "    w_enc = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(w)\n",
    "\n",
    "    # Pool\n",
    "    b_pool = layers.GlobalAveragePooling1D()(b_enc)\n",
    "    w_pool = layers.GlobalAveragePooling1D()(w_enc)\n",
    "\n",
    "    x = layers.Concatenate()([b_pool, w_pool])\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[board_in, word_in], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\"),\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline = make_baseline_model()\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Prepare tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xy(pos_list, neg_list):\n",
    "    boards = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    for b, w, _path in pos_list:\n",
    "        boards.append(encode_board(b))\n",
    "        words.append(encode_word(w))\n",
    "        labels.append(1)\n",
    "    for b, w in neg_list:\n",
    "        boards.append(encode_board(b))\n",
    "        words.append(encode_word(w))\n",
    "        labels.append(0)\n",
    "    Xb = np.stack(boards, axis=0)\n",
    "    Xw = np.stack(words, axis=0)\n",
    "    y = np.array(labels, dtype=np.float32)\n",
    "    return (Xb, Xw), y\n",
    "\n",
    "\n",
    "X_train, y_train = build_xy(ds[\"train_pos\"], ds[\"train_neg\"])\n",
    "X_val, y_val = build_xy(ds[\"val_pos\"], ds[\"val_neg\"])\n",
    "X_test, y_test = build_xy(ds[\"test_pos\"], ds[\"test_neg\"])\n",
    "\n",
    "for name, (X, y) in {\n",
    "    \"train\": (X_train, y_train),\n",
    "    \"val\": (X_val, y_val),\n",
    "    \"test\": (X_test, y_test),\n",
    "}.items():\n",
    "    print(name, X[0].shape, X[1].shape, y.shape, f\"pos={y.sum()} neg={len(y) - y.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Train baseline (small epochs to keep CPU time reasonable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = baseline.fit(\n",
    "    {\"board\": X_train[0], \"word\": X_train[1]},\n",
    "    y_train,\n",
    "    validation_data=({\"board\": X_val[0], \"word\": X_val[1]}, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "test_metrics = baseline.evaluate({\"board\": X_test[0], \"word\": X_test[1]}, y_test, verbose=0)\n",
    "print(\"\\nTest metrics:\")\n",
    "for name, val in zip(baseline.metrics_names, test_metrics):\n",
    "    print(f\"  {name}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Save dataset artefacts for reuse\n",
    "We store arrays and the alphabet mapping so later notebooks can load them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path(\"../data\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.savez_compressed(\n",
    "    outdir / \"boggle_3x3_dataset.npz\",\n",
    "    Xb_train=X_train[0],\n",
    "    Xw_train=X_train[1],\n",
    "    y_train=y_train,\n",
    "    Xb_val=X_val[0],\n",
    "    Xw_val=X_val[1],\n",
    "    y_val=y_val,\n",
    "    Xb_test=X_test[0],\n",
    "    Xw_test=X_test[1],\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "with open(outdir / \"alphabet.json\", \"w\") as f:\n",
    "    json.dump({\"alphabet\": ALPH, \"c2i\": C2I}, f)\n",
    "\n",
    "print(f\"Saved data to {outdir.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## What’s next (Task B)\n",
    "In **Task B**, we’ll train a **pointer-style next-move policy** that emits an explicit path (cell indices), with masking of illegal moves and teacher forcing from oracle paths. That model must respect the **geometry** of the grid, not just the spelling.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "simulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
