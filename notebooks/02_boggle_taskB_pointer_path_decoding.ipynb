{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task B — Supervised Path Decoding (Pointer-style, Keras)\n",
    "\n",
    "**Goal:** Train a model that, given a 3×3 board and a **query word (3–5 letters)**, emits a **valid path** through the grid (sequence of 0..8 indices) or declares “no path”.\n",
    "\n",
    "We implement a compact, step-wise decoder with **teacher forcing**:\n",
    "- At step *t*, score each of the 9 cells.\n",
    "- Apply masks for: used cells and non-adjacent moves from the previous pick.\n",
    "- Cross-entropy loss against the oracle path for known words.\n",
    "- Greedy decoding at inference.\n",
    "\n",
    "This forces the model to internalise **geometry + spelling**, not just dictionary membership.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0. Setup & data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "\n",
    "CWD = Path().cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "data = np.load(CWD / \"../data/boggle_3x3_dataset.npz\")\n",
    "with open(CWD / \"../data/alphabet.json\") as f:\n",
    "    alph_info = json.load(f)\n",
    "ALPH = alph_info[\"alphabet\"]\n",
    "C2I = {c: i + 1 for i, c in enumerate(ALPH)}\n",
    "PAD = 0\n",
    "MAX_WORD_LEN = 5\n",
    "BOARD_LEN = 9\n",
    "\n",
    "DICT_CANDIDATES = [\n",
    "    \"/usr/share/dict/british-english\",\n",
    "    \"/usr/share/dict/words\",\n",
    "    \"/usr/share/dict/american-english\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_words(paths=DICT_CANDIDATES, min_len=3, max_len=5):\n",
    "    words = set()\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for w in f:\n",
    "                    w = w.strip().lower()\n",
    "                    if w.isalpha() and min_len <= len(w) <= max_len:\n",
    "                        words.add(w)\n",
    "    if not words:\n",
    "        words = {\n",
    "            \"cat\",\n",
    "            \"cats\",\n",
    "            \"cater\",\n",
    "            \"art\",\n",
    "            \"arts\",\n",
    "            \"rat\",\n",
    "            \"rate\",\n",
    "            \"rates\",\n",
    "            \"tar\",\n",
    "            \"tars\",\n",
    "            \"tire\",\n",
    "            \"tired\",\n",
    "            \"ride\",\n",
    "            \"rides\",\n",
    "            \"ear\",\n",
    "            \"ears\",\n",
    "            \"earl\",\n",
    "            \"ale\",\n",
    "            \"ales\",\n",
    "            \"tea\",\n",
    "            \"teas\",\n",
    "            \"eat\",\n",
    "            \"eats\",\n",
    "            \"seat\",\n",
    "            \"sear\",\n",
    "            \"scar\",\n",
    "            \"care\",\n",
    "            \"cared\",\n",
    "        }\n",
    "    return sorted(words)\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    __slots__ = (\"children\", \"is_word\")\n",
    "\n",
    "    def __init__(self):\n",
    "        self.children, self.is_word = {}, False\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, words):\n",
    "        self.root = TrieNode()\n",
    "        for w in words:\n",
    "            self.add(w)\n",
    "\n",
    "    def add(self, w):\n",
    "        n = self.root\n",
    "        for ch in w:\n",
    "            n = n.children.setdefault(ch, TrieNode())\n",
    "        n.is_word = True\n",
    "\n",
    "    def has_prefix(self, prefix):\n",
    "        n = self.root\n",
    "        for ch in prefix:\n",
    "            n = n.children.get(ch)\n",
    "            if n is None:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def is_word(self, w):\n",
    "        n = self.root\n",
    "        for ch in w:\n",
    "            n = n.children.get(ch)\n",
    "            if n is None:\n",
    "                return False\n",
    "        return n.is_word\n",
    "\n",
    "\n",
    "WORDS = load_words()\n",
    "TRIE = Trie(WORDS)\n",
    "\n",
    "IDX2RC = [(r, c) for r in range(3) for c in range(3)]\n",
    "RC2IDX = {rc: i for i, rc in enumerate(IDX2RC)}\n",
    "\n",
    "\n",
    "def neighbours(idx, use_diagonals=True):\n",
    "    r, c = IDX2RC[idx]\n",
    "    nbrs = []\n",
    "    for dr in (-1, 0, 1):\n",
    "        for dc in (-1, 0, 1):\n",
    "            if dr == 0 and dc == 0:\n",
    "                continue\n",
    "            if not use_diagonals and abs(dr) + abs(dc) != 1:\n",
    "                continue\n",
    "            rr, cc = r + dr, c + dc\n",
    "            if 0 <= rr < 3 and 0 <= cc < 3:\n",
    "                nbrs.append(RC2IDX[(rr, cc)])\n",
    "    return nbrs\n",
    "\n",
    "\n",
    "def encode_word(w):\n",
    "    arr = [C2I.get(ch, PAD) for ch in w.lower()]\n",
    "    if len(arr) < MAX_WORD_LEN:\n",
    "        arr += [PAD] * (MAX_WORD_LEN - len(arr))\n",
    "    return np.array(arr, dtype=np.int32)\n",
    "\n",
    "\n",
    "def encode_board(bstr):\n",
    "    arr = [C2I.get(ch, PAD) for ch in bstr.lower()]\n",
    "    return np.array(arr, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Rebuild training triples from oracle\n",
    "We need **(board, word, path)**. We regenerate oracle paths for the boards present in the Task‑A splits to preserve the board‑level split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull boards from the saved arrays\n",
    "Xb_tr = data[\"Xb_train\"]\n",
    "Xb_va = data[\"Xb_val\"]\n",
    "Xb_te = data[\"Xb_test\"]\n",
    "\n",
    "\n",
    "def decode_board_tokens(tokens):\n",
    "    inv = {v: k for k, v in C2I.items()}\n",
    "    return \"\".join(inv.get(int(t), \"?\") for t in tokens)\n",
    "\n",
    "\n",
    "train_boards = {decode_board_tokens(x) for x in Xb_tr}\n",
    "val_boards = {decode_board_tokens(x) for x in Xb_va}\n",
    "test_boards = {decode_board_tokens(x) for x in Xb_te}\n",
    "\n",
    "\n",
    "# Oracle\n",
    "def oracle_words_on_board(board, use_diagonals=True, min_len=3, max_len=5):\n",
    "    board = list(board)\n",
    "    results = []\n",
    "    used = [False] * 9\n",
    "\n",
    "    def dfs(idx, prefix, path):\n",
    "        ch = board[idx]\n",
    "        new_prefix = prefix + ch\n",
    "        if not TRIE.has_prefix(new_prefix):\n",
    "            return\n",
    "        used[idx] = True\n",
    "        path.append(idx)\n",
    "        if TRIE.is_word(new_prefix) and (min_len <= len(new_prefix) <= max_len):\n",
    "            results.append((\"\".join(new_prefix), path.copy()))\n",
    "        for nb in neighbours(idx, use_diagonals=use_diagonals):\n",
    "            if not used[nb]:\n",
    "                dfs(nb, new_prefix, path)\n",
    "        path.pop()\n",
    "        used[idx] = False\n",
    "\n",
    "    for start in range(9):\n",
    "        dfs(start, \"\", [])\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_triples(board_set):\n",
    "    triples = []\n",
    "    for b in board_set:\n",
    "        pairs = oracle_words_on_board(b, use_diagonals=True)\n",
    "        for w, p in pairs:\n",
    "            triples.append((b, w, p))\n",
    "    return triples\n",
    "\n",
    "\n",
    "triples_tr = build_triples(train_boards)\n",
    "triples_va = build_triples(val_boards)\n",
    "triples_te = build_triples(test_boards)\n",
    "print(\"Triples:\", len(triples_tr), len(triples_va), len(triples_te))\n",
    "print(\"Example:\", triples_tr[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Vectorise for step-wise training\n",
    "For each triple *(b, w, path)* we create **T** training steps where *T=len(w)*. At step *t*, the target is `path[t]`.\n",
    "\n",
    "We will implement a custom training loop to apply **masks** per-example & per-step:\n",
    "- **used mask** (no repeats),\n",
    "- **adjacency mask** (must be neighbour of previous index, except at t=0 where any cell is allowed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPH_SIZE = len(C2I) + 1\n",
    "EMB = 64\n",
    "\n",
    "\n",
    "def preencode(triples):\n",
    "    enc = []\n",
    "    for b, w, p in triples:\n",
    "        enc.append((encode_board(b), encode_word(w), np.array(p, dtype=np.int32)))\n",
    "    return enc\n",
    "\n",
    "\n",
    "enc_tr = preencode(triples_tr)\n",
    "enc_va = preencode(triples_va)\n",
    "enc_te = preencode(triples_te)\n",
    "\n",
    "ADJ = np.zeros((9, 9), dtype=np.float32)\n",
    "for i in range(9):\n",
    "    for j in neighbours(i, use_diagonals=True):\n",
    "        ADJ[i, j] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Model definition (cell-scoring network)\n",
    "\n",
    "Given:\n",
    "- **board_emb**: (9, D)\n",
    "- **word_emb**: (5, D), we will gather char *t* and also use a pooled word embedding.\n",
    "- **prev_index**: index of previous cell (or -1 for start).\n",
    "- **step t**: optional step embedding.\n",
    "\n",
    "We compute a score for each of the 9 cells using a small MLP over concatenated features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellScorer(keras.Model):\n",
    "    def __init__(self, vocab_size, d_model=EMB):\n",
    "        super().__init__()\n",
    "        self.emb = layers.Embedding(input_dim=vocab_size, output_dim=d_model, mask_zero=True)\n",
    "        self.pos_emb = layers.Embedding(\n",
    "            input_dim=10, output_dim=d_model\n",
    "        )  # 9 positions + 1 for \"no prev\"\n",
    "        self.step_emb = layers.Embedding(input_dim=6, output_dim=d_model)  # steps 0..5\n",
    "        self.w_encoder = layers.Bidirectional(layers.LSTM(d_model // 2, return_sequences=True))\n",
    "        self.ff = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(128, activation=\"relu\"),\n",
    "                layers.Dense(64, activation=\"relu\"),\n",
    "                layers.Dense(1),  # per-cell score\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, board_tokens, word_tokens, prev_index, step_t, training=False):\n",
    "        B = tf.shape(board_tokens)[0]\n",
    "        b_emb = self.emb(board_tokens)  # (B,9,D)\n",
    "        w_emb = self.emb(word_tokens)  # (B,5,D)\n",
    "        w_seq = self.w_encoder(w_emb, training=training)  # (B,5,D)\n",
    "        w_pool = tf.reduce_mean(w_seq, axis=1)  # (B,D)\n",
    "        cur_idx = tf.stack([tf.range(B), step_t], axis=1)\n",
    "        cur_char = tf.gather_nd(w_seq, cur_idx)  # (B,D)\n",
    "        prev_index = tf.clip_by_value(prev_index, -1, 8)\n",
    "        prev_pos_emb = self.pos_emb(prev_index + 1)  # shift by +1 so -1 -> 0\n",
    "        step_e = self.step_emb(step_t)\n",
    "        w_pool_t = tf.tile(tf.expand_dims(w_pool, axis=1), [1, 9, 1])  # (B,9,D)\n",
    "        cur_char_t = tf.tile(tf.expand_dims(cur_char, axis=1), [1, 9, 1])\n",
    "        prev_pos_t = tf.tile(tf.expand_dims(prev_pos_emb, axis=1), [1, 9, 1])\n",
    "        step_tiled = tf.tile(tf.expand_dims(step_e, axis=1), [1, 9, 1])\n",
    "        x = tf.concat([b_emb, w_pool_t, cur_char_t, prev_pos_t, step_tiled], axis=-1)  # (B,9,5D)\n",
    "        scores = self.ff(x)  # (B,9,1)\n",
    "        return tf.squeeze(scores, axis=-1)  # (B,9)\n",
    "\n",
    "\n",
    "scorer = CellScorer(vocab_size=ALPH_SIZE)\n",
    "dummy_b = tf.zeros((2, 9), dtype=tf.int32)\n",
    "dummy_w = tf.zeros((2, 5), dtype=tf.int32)\n",
    "_ = scorer(dummy_b, dummy_w, prev_index=tf.constant([-1, -1]), step_t=tf.constant([0, 0]))\n",
    "scorer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Training loop with masking\n",
    "- **used_mask**: 1 for free cells, 0 for used.\n",
    "- **adj_mask**: 1 for neighbours of prev (or all ones at t=0).\n",
    "\n",
    "Final mask = used_mask ∧ adj_mask. Illegal cells get logits `-1e9`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "optimizer = keras.optimizers.Adam(LR)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(board_tokens, word_tokens, path_targets):\n",
    "    B = tf.shape(board_tokens)[0]\n",
    "    T = tf.shape(word_tokens)[1]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_count = 0.0\n",
    "\n",
    "    used = tf.zeros((B, 9), dtype=tf.float32)\n",
    "    prev = tf.fill((B,), -1)\n",
    "    for t in tf.range(T):\n",
    "        tgt = path_targets[:, t]\n",
    "        active = tf.where(tgt >= 0)[:, 0]\n",
    "        if tf.shape(active)[0] == 0:\n",
    "            break\n",
    "\n",
    "        b_tok = tf.gather(board_tokens, active)\n",
    "        w_tok = tf.gather(word_tokens, active)\n",
    "        tgt_a = tf.gather(tgt, active)\n",
    "        used_a = tf.gather(used, active)\n",
    "        prev_a = tf.gather(prev, active)\n",
    "        step_a = tf.fill((tf.shape(active)[0],), t)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = scorer(b_tok, w_tok, prev_a, step_a, training=True)\n",
    "\n",
    "            used_mask = 1.0 - used_a\n",
    "            if t == 0:\n",
    "                adj_mask = tf.ones_like(logits)\n",
    "            else:\n",
    "                adj = tf.gather(tf.constant(ADJ), prev_a)\n",
    "                adj_mask = adj\n",
    "            mask = used_mask * adj_mask\n",
    "            masked_logits = logits + (1.0 - mask) * (-1e9)\n",
    "\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tgt_a, logits=masked_logits)\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(loss, scorer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, scorer.trainable_variables))\n",
    "\n",
    "        total_loss += loss * tf.cast(tf.shape(active)[0], tf.float32)\n",
    "        total_count += tf.cast(tf.shape(active)[0], tf.float32)\n",
    "\n",
    "        upd = tf.one_hot(tgt_a, depth=9, dtype=tf.float32)\n",
    "        full_upd = tf.tensor_scatter_nd_update(used, tf.reshape(active, (-1, 1)), used_a + upd)\n",
    "        used = full_upd\n",
    "        prev = tf.tensor_scatter_nd_update(prev, tf.reshape(active, (-1, 1)), tgt_a)\n",
    "\n",
    "    return total_loss / (total_count + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Build padded path tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(enc, batch_size=64):\n",
    "    random.shuffle(enc)\n",
    "    for i in range(0, len(enc), batch_size):\n",
    "        batch = enc[i : i + batch_size]\n",
    "        b_tok = np.stack([b for b, w, p in batch], axis=0)\n",
    "        w_tok = np.stack([w for b, w, p in batch], axis=0)\n",
    "        paths = []\n",
    "        for b, w, p in batch:\n",
    "            arr = np.full((MAX_WORD_LEN,), -1, dtype=np.int32)\n",
    "            arr[: len(p)] = p\n",
    "            paths.append(arr)\n",
    "        y = np.stack(paths, axis=0)\n",
    "        yield b_tok, w_tok, y\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    losses = []\n",
    "    for b_tok, w_tok, y in batchify(enc_tr, batch_size=64):\n",
    "        loss = train_step(tf.constant(b_tok), tf.constant(w_tok), tf.constant(y))\n",
    "        losses.append(float(loss.numpy()))\n",
    "    print(f\"Epoch {epoch}: mean loss {np.mean(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Greedy decoding & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(board_tokens, word_tokens):\n",
    "    B = board_tokens.shape[0]\n",
    "    paths = []\n",
    "    for i in range(B):\n",
    "        b_tok = tf.constant(board_tokens[i : i + 1])\n",
    "        w_tok = tf.constant(word_tokens[i : i + 1])\n",
    "        used = tf.zeros((1, 9), dtype=tf.float32)\n",
    "        prev = tf.constant([-1], dtype=tf.int32)\n",
    "        cur_path = []\n",
    "        for t in range(MAX_WORD_LEN):\n",
    "            logits = scorer(b_tok, w_tok, prev, tf.constant([t]), training=False)\n",
    "            used_mask = 1.0 - used\n",
    "            if t == 0:\n",
    "                adj_mask = tf.ones_like(logits)\n",
    "            else:\n",
    "                adj_mask = tf.gather(tf.constant(ADJ), prev)\n",
    "            mask = used_mask * adj_mask\n",
    "            masked_logits = logits + (1.0 - mask) * (-1e9)\n",
    "            idx = int(tf.argmax(masked_logits[0]).numpy())\n",
    "            cur_path.append(idx)\n",
    "            used = used + tf.one_hot([idx], depth=9, dtype=tf.float32)\n",
    "            prev = tf.constant([idx], dtype=tf.int32)\n",
    "        paths.append(cur_path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def evaluate_exact(triples, n_samples=500):\n",
    "    sample = random.sample(triples, min(n_samples, len(triples)))\n",
    "    correct = 0\n",
    "    for b, w, p in sample:\n",
    "        b_tok = encode_board(b)[None, :]\n",
    "        w_tok = encode_word(w)[None, :]\n",
    "        pred = greedy_decode(b_tok, w_tok)[0][: len(p)]\n",
    "        correct += int(pred == p)\n",
    "    return correct / len(sample)\n",
    "\n",
    "\n",
    "acc_tr = evaluate_exact(triples_tr, n_samples=300)\n",
    "acc_va = evaluate_exact(triples_va, n_samples=200)\n",
    "acc_te = evaluate_exact(triples_te, n_samples=200)\n",
    "print(f\"Exact path match — Train:{acc_tr:.3f}  Val:{acc_va:.3f}  Test:{acc_te:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Visualise a board and predicted path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small helper to print a board and overlay the predicted path\n",
    "IDX2RC = [(r, c) for r in range(3) for c in range(3)]\n",
    "\n",
    "\n",
    "def show_board_with_path(board_str, path):\n",
    "    print(\"\\n\".join(\" \".join(board_str[r * 3 : (r + 1) * 3]) for r in range(3)))\n",
    "    print(\"Path:\", path)\n",
    "\n",
    "\n",
    "example = random.choice(triples_te)\n",
    "b, w, p = example\n",
    "print(\"Word:\", w)\n",
    "pred_path = greedy_decode(encode_board(b)[None, :], encode_word(w)[None, :])[0][: len(p)]\n",
    "show_board_with_path(b, pred_path)\n",
    "print(\"Oracle:\", p, \"Pred:\", pred_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- The model relies on **masking** to enforce legality. This is an **inductive bias**, not cheating; we still need to learn *where* to move to match the word.\n",
    "- You can try relative position features, deeper encoders, or beam search to improve exact-match accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "simulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
